
@article{szegedy_intriguing_2013,
	title = {Intriguing properties of neural networks},
	url = {http://arxiv.org/abs/1312.6199},
	abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.},
	language = {en},
	urldate = {2018-11-29},
	journal = {arXiv:1312.6199 [cs]},
	author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
	month = dec,
	year = {2013},
	note = {arXiv: 1312.6199},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Szegedy et al. - 2013 - Intriguing properties of neural networks.pdf:files/100/Szegedy et al. - 2013 - Intriguing properties of neural networks.pdf:application/pdf}
}

@article{fogla_polymorphic_nodate,
	title = {Polymorphic {Blending} {Attacks}},
	abstract = {A very effective means to evade signature-based intrusion detection systems (IDS) is to employ polymorphic techniques to generate attack instances that do not share a ﬁxed signature. Anomaly-based intrusion detection systems provide good defense because existing polymorphic techniques can make the attack instances look different from each other, but cannot make them look like normal. In this paper we introduce a new class of polymorphic attacks, called polymorphic blending attacks, that can effectively evade byte frequencybased network anomaly IDS by carefully matching the statistics of the mutated attack instances to the normal proﬁles. The proposed polymorphic blending attacks can be viewed as a subclass of the mimicry attacks. We take a systematic approach to the problem and formally describe the algorithms and steps required to carry out such attacks. We not only show that such attacks are feasible but also analyze the hardness of evasion under different circumstances. We present detailed techniques using PAYL, a byte frequency-based anomaly IDS, as a case study and demonstrate that these attacks are indeed feasible. We also provide some insight into possible countermeasures that can be used as defense.},
	language = {en},
	author = {Fogla, Prahlad and Sharif, Monirul and Perdisci, Roberto and Kolesnikov, Oleg and Lee, Wenke},
	pages = {16},
	file = {Fogla et al. - Polymorphic Blending Attacks.pdf:files/101/Fogla et al. - Polymorphic Blending Attacks.pdf:application/pdf}
}

@article{biggio_evasion_2013,
	title = {Evasion {Attacks} against {Machine} {Learning} at {Test} {Time}},
	volume = {7908},
	url = {http://arxiv.org/abs/1708.06131},
	doi = {10.1007/978-3-642-40994-3_25},
	abstract = {In security-sensitive applications, the success of machine learning depends on a thorough vetting of their resistance to adversarial data. In one pertinent, well-motivated attack scenario, an adversary may attempt to evade a deployed system at test time by carefully manipulating attack samples. In this work, we present a simple but effective gradient-based approach that can be exploited to systematically assess the security of several, widely-used classification algorithms against evasion attacks. Following a recently proposed framework for security evaluation, we simulate attack scenarios that exhibit different risk levels for the classifier by increasing the attacker's knowledge of the system and her ability to manipulate attack samples. This gives the classifier designer a better picture of the classifier performance under evasion attacks, and allows him to perform a more informed model selection (or parameter setting). We evaluate our approach on the relevant security task of malware detection in PDF files, and show that such systems can be easily evaded. We also sketch some countermeasures suggested by our analysis.},
	urldate = {2018-11-29},
	journal = {arXiv:1708.06131 [cs]},
	author = {Biggio, Battista and Corona, Igino and Maiorca, Davide and Nelson, Blaine and Srndic, Nedim and Laskov, Pavel and Giacinto, Giorgio and Roli, Fabio},
	year = {2013},
	note = {arXiv: 1708.06131},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	pages = {387--402},
	annote = {Comment: In this paper, in 2013, we were the first to introduce the notion of evasion attacks (adversarial examples) created with high confidence (instead of minimum-distance misclassifications), and the notion of surrogate learners (substitute models). These two concepts are now widely re-used in developing attacks against deep networks (even if not always referring to the ideas reported in this work). arXiv admin note: text overlap with arXiv:1401.7727
If the discriminator is differentiable gradient descent method is applicable. This paper looks at MNIST and PDF files.
Look at previous methods such as min-max and Nash equilibrium
Look at some mimicry stuff
Non-invertible features
 },
	file = {arXiv\:1708.06131 PDF:files/104/Biggio et al. - 2013 - Evasion Attacks against Machine Learning at Test T.pdf:application/pdf;arXiv.org Snapshot:files/103/1708.html:text/html}
}

@article{hinton_distilling_2015,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	url = {http://arxiv.org/abs/1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	urldate = {2018-11-27},
	journal = {arXiv:1503.02531 [cs, stat]},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.02531},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: NIPS 2014 Deep Learning Workshop},
	file = {arXiv\:1503.02531 PDF:files/107/Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf:application/pdf;arXiv.org Snapshot:files/106/1503.html:text/html}
}

@article{goodfellow_explaining_2014,
	title = {Explaining and {Harnessing} {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1412.6572},
	abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
	urldate = {2018-11-27},
	journal = {arXiv:1412.6572 [cs, stat]},
	author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.6572},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1412.6572 PDF:files/110/Goodfellow et al. - 2014 - Explaining and Harnessing Adversarial Examples.pdf:application/pdf;arXiv.org Snapshot:files/109/1412.html:text/html}
}

@article{alzantot_did_2018,
	title = {Did you hear that? {Adversarial} {Examples} {Against} {Automatic} {Speech} {Recognition}},
	shorttitle = {Did you hear that?},
	url = {http://arxiv.org/abs/1801.00554},
	abstract = {Speech is a common and effective way of communication between humans, and modern consumer devices such as smartphones and home hubs are equipped with deep learning based accurate automatic speech recognition to enable natural interaction between humans and machines. Recently, researchers have demonstrated powerful attacks against machine learning models that can fool them to produceincorrect results. However, nearly all previous research in adversarial attacks has focused on image recognition and object detection models. In this short paper, we present a first of its kind demonstration of adversarial attacks against speech classification model. Our algorithm performs targeted attacks with 87\% success by adding small background noise without having to know the underlying model parameter and architecture. Our attack only changes the least significant bits of a subset of audio clip samples, and the noise does not change 89\% the human listener's perception of the audio clip as evaluated in our human study.},
	urldate = {2018-11-27},
	journal = {arXiv:1801.00554 [cs]},
	author = {Alzantot, Moustafa and Balaji, Bharathan and Srivastava, Mani},
	month = jan,
	year = {2018},
	note = {arXiv: 1801.00554},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Computation and Language},
	annote = {Comment: Published in NIPS 2017 Machine Deception workshop},
	file = {arXiv\:1801.00554 PDF:files/112/Alzantot et al. - 2018 - Did you hear that Adversarial Examples Against Au.pdf:application/pdf;arXiv.org Snapshot:files/111/1801.html:text/html}
}

@article{carlini_audio_2018,
	title = {Audio {Adversarial} {Examples}: {Targeted} {Attacks} on {Speech}-to-{Text}},
	shorttitle = {Audio {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1801.01944},
	abstract = {We construct targeted audio adversarial examples on automatic speech recognition. Given any audio waveform, we can produce another that is over 99.9\% similar, but transcribes as any phrase we choose (recognizing up to 50 characters per second of audio). We apply our white-box iterative optimization-based attack to Mozilla's implementation DeepSpeech end-to-end, and show it has a 100\% success rate. The feasibility of this attack introduce a new domain to study adversarial examples.},
	urldate = {2018-11-27},
	journal = {arXiv:1801.01944 [cs]},
	author = {Carlini, Nicholas and Wagner, David},
	month = jan,
	year = {2018},
	note = {arXiv: 1801.01944},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security, Computer Science - Artificial Intelligence},
	file = {arXiv\:1801.01944 PDF:files/115/Carlini and Wagner - 2018 - Audio Adversarial Examples Targeted Attacks on Sp.pdf:application/pdf;arXiv.org Snapshot:files/114/1801.html:text/html}
}

@article{makhzani_adversarial_2015,
	title = {Adversarial {Autoencoders}},
	url = {http://arxiv.org/abs/1511.05644},
	abstract = {In this paper, we propose the "adversarial autoencoder" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.},
	urldate = {2018-11-27},
	journal = {arXiv:1511.05644 [cs]},
	author = {Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.05644},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv\:1511.05644 PDF:files/120/Makhzani et al. - 2015 - Adversarial Autoencoders.pdf:application/pdf;arXiv.org Snapshot:files/119/1511.html:text/html}
}

@article{papernot_transferability_2016,
	title = {Transferability in {Machine} {Learning}: from {Phenomena} to {Black}-{Box} {Attacks} using {Adversarial} {Samples}},
	shorttitle = {Transferability in {Machine} {Learning}},
	url = {http://arxiv.org/abs/1605.07277},
	abstract = {Many machine learning models are vulnerable to adversarial examples: inputs that are specially crafted to cause a machine learning model to produce an incorrect output. Adversarial examples that affect one model often affect another model, even if the two models have different architectures or were trained on different training sets, so long as both models were trained to perform the same task. An attacker may therefore train their own substitute model, craft adversarial examples against the substitute, and transfer them to a victim model, with very little information about the victim. Recent work has further developed a technique that uses the victim model as an oracle to label a synthetic training set for the substitute, so the attacker need not even collect a training set to mount the attack. We extend these recent techniques using reservoir sampling to greatly enhance the efficiency of the training procedure for the substitute model. We introduce new transferability attacks between previously unexplored (substitute, victim) pairs of machine learning model classes, most notably SVMs and decision trees. We demonstrate our attacks on two commercial machine learning classification systems from Amazon (96.19\% misclassification rate) and Google (88.94\%) using only 800 queries of the victim model, thereby showing that existing machine learning approaches are in general vulnerable to systematic black-box attacks regardless of their structure.},
	urldate = {2018-11-27},
	journal = {arXiv:1605.07277 [cs]},
	author = {Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian},
	month = may,
	year = {2016},
	note = {arXiv: 1605.07277},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv\:1605.07277 PDF:files/122/Papernot et al. - 2016 - Transferability in Machine Learning from Phenomen.pdf:application/pdf;arXiv.org Snapshot:files/121/1605.html:text/html}
}

@article{gu_towards_2014,
	title = {Towards {Deep} {Neural} {Network} {Architectures} {Robust} to {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1412.5068},
	abstract = {Recent work has shown deep neural networks (DNNs) to be highly susceptible to well-designed, small perturbations at the input layer, or so-called adversarial examples. Taking images as an example, such distortions are often imperceptible, but can result in 100\% mis-classification for a state of the art DNN. We study the structure of adversarial examples and explore network topology, pre-processing and training strategies to improve the robustness of DNNs. We perform various experiments to assess the removability of adversarial examples by corrupting with additional noise and pre-processing with denoising autoencoders (DAEs). We find that DAEs can remove substantial amounts of the adversarial noise. How- ever, when stacking the DAE with the original DNN, the resulting network can again be attacked by new adversarial examples with even smaller distortion. As a solution, we propose Deep Contractive Network, a model with a new end-to-end training procedure that includes a smoothness penalty inspired by the contractive autoencoder (CAE). This increases the network robustness to adversarial examples, without a significant performance penalty.},
	urldate = {2018-11-27},
	journal = {arXiv:1412.5068 [cs]},
	author = {Gu, Shixiang and Rigazio, Luca},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.5068},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1412.5068 PDF:files/124/Gu and Rigazio - 2014 - Towards Deep Neural Network Architectures Robust t.pdf:application/pdf;arXiv.org Snapshot:files/123/1412.html:text/html}
}

@article{papernot_distillation_2015,
	title = {Distillation as a {Defense} to {Adversarial} {Perturbations} against {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.04508},
	abstract = {Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content filters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95\% to less than 0.5\% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 10{\textasciicircum}30. We also find that distillation increases the average minimum number of features that need to be modified to create adversarial samples by about 800\% on one of the DNNs we tested.},
	urldate = {2018-11-27},
	journal = {arXiv:1511.04508 [cs, stat]},
	author = {Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.04508},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Cryptography and Security, Statistics - Machine Learning},
	file = {arXiv\:1511.04508 PDF:files/126/Papernot et al. - 2015 - Distillation as a Defense to Adversarial Perturbat.pdf:application/pdf;arXiv.org Snapshot:files/125/1511.html:text/html}
}

@article{madry_towards_2017,
	title = {Towards {Deep} {Learning} {Models} {Resistant} to {Adversarial} {Attacks}},
	url = {http://arxiv.org/abs/1706.06083},
	abstract = {Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models.},
	urldate = {2018-11-27},
	journal = {arXiv:1706.06083 [cs, stat]},
	author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.06083},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv\:1706.06083 PDF:files/128/Madry et al. - 2017 - Towards Deep Learning Models Resistant to Adversar.pdf:application/pdf;arXiv.org Snapshot:files/127/1706.html:text/html}
}

@article{papernot_limitations_2015,
	title = {The {Limitations} of {Deep} {Learning} in {Adversarial} {Settings}},
	url = {http://arxiv.org/abs/1511.07528},
	abstract = {Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples: inputs crafted by adversaries with the intent of causing deep neural networks to misclassify. In this work, we formalize the space of adversaries against deep neural networks (DNNs) and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs. In an application to computer vision, we show that our algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97\% adversarial success rate while only modifying on average 4.02\% of the input features per sample. We then evaluate the vulnerability of different sample classes to adversarial perturbations by defining a hardness measure. Finally, we describe preliminary work outlining defenses against adversarial samples by defining a predictive measure of distance between a benign input and a target classification.},
	urldate = {2018-11-27},
	journal = {arXiv:1511.07528 [cs, stat]},
	author = {Papernot, Nicolas and McDaniel, Patrick and Jha, Somesh and Fredrikson, Matt and Celik, Z. Berkay and Swami, Ananthram},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.07528},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Cryptography and Security, Statistics - Machine Learning},
	annote = {Comment: Accepted to the 1st IEEE European Symposium on Security \& Privacy, IEEE 2016. Saarbrucken, Germany},
	file = {arXiv\:1511.07528 PDF:files/130/Papernot et al. - 2015 - The Limitations of Deep Learning in Adversarial Se.pdf:application/pdf;arXiv.org Snapshot:files/129/1511.html:text/html}
}

@article{metzen_detecting_2017,
	title = {On {Detecting} {Adversarial} {Perturbations}},
	url = {http://arxiv.org/abs/1702.04267},
	abstract = {Machine learning and deep learning in particular has advanced tremendously on perceptual tasks in recent years. However, it remains vulnerable against adversarial perturbations of the input that have been crafted specifically to fool the system while being quasi-imperceptible to a human. In this work, we propose to augment deep neural networks with a small "detector" subnetwork which is trained on the binary classification task of distinguishing genuine data from data containing adversarial perturbations. Our method is orthogonal to prior work on addressing adversarial perturbations, which has mostly focused on making the classification network itself more robust. We show empirically that adversarial perturbations can be detected surprisingly well even though they are quasi-imperceptible to humans. Moreover, while the detectors have been trained to detect only a specific adversary, they generalize to similar and weaker adversaries. In addition, we propose an adversarial attack that fools both the classifier and the detector and a novel training procedure for the detector that counteracts this attack.},
	urldate = {2018-12-05},
	journal = {arXiv:1702.04267 [cs, stat]},
	author = {Metzen, Jan Hendrik and Genewein, Tim and Fischer, Volker and Bischoff, Bastian},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.04267},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Final version for ICLR2017 (see https://openreview.net/forum?id=SJzCSf9xg¬eId=SJzCSf9xg)},
	file = {arXiv\:1702.04267 PDF:files/203/Metzen et al. - 2017 - On Detecting Adversarial Perturbations.pdf:application/pdf;arXiv.org Snapshot:files/204/1702.html:text/html}
}

@article{feinman_detecting_2017,
	title = {Detecting {Adversarial} {Samples} from {Artifacts}},
	url = {http://arxiv.org/abs/1703.00410},
	abstract = {Deep neural networks (DNNs) are powerful nonlinear architectures that are known to be robust to random perturbations of the input. However, these models are vulnerable to adversarial perturbations--small input changes crafted explicitly to fool the model. In this paper, we ask whether a DNN can distinguish adversarial samples from their normal and noisy counterparts. We investigate model confidence on adversarial samples by looking at Bayesian uncertainty estimates, available in dropout neural networks, and by performing density estimation in the subspace of deep features learned by the model. The result is a method for implicit adversarial detection that is oblivious to the attack algorithm. We evaluate this method on a variety of standard datasets including MNIST and CIFAR-10 and show that it generalizes well across different architectures and attacks. Our findings report that 85-93\% ROC-AUC can be achieved on a number of standard classification tasks with a negative class that consists of both normal and noisy samples.},
	urldate = {2018-12-05},
	journal = {arXiv:1703.00410 [cs, stat]},
	author = {Feinman, Reuben and Curtin, Ryan R. and Shintre, Saurabh and Gardner, Andrew B.},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.00410},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Submitted to ICML 2017},
	file = {arXiv\:1703.00410 PDF:files/207/Feinman et al. - 2017 - Detecting Adversarial Samples from Artifacts.pdf:application/pdf;arXiv.org Snapshot:files/208/1703.html:text/html}
}

@article{carlini_adversarial_2017,
	title = {Adversarial {Examples} {Are} {Not} {Easily} {Detected}: {Bypassing} {Ten} {Detection} {Methods}},
	shorttitle = {Adversarial {Examples} {Are} {Not} {Easily} {Detected}},
	url = {http://arxiv.org/abs/1705.07263},
	abstract = {Neural networks are known to be vulnerable to adversarial examples: inputs that are close to natural inputs but classified incorrectly. In order to better understand the space of adversarial examples, we survey ten recent proposals that are designed for detection and compare their efficacy. We show that all can be defeated by constructing new loss functions. We conclude that adversarial examples are significantly harder to detect than previously appreciated, and the properties believed to be intrinsic to adversarial examples are in fact not. Finally, we propose several simple guidelines for evaluating future proposed defenses.},
	urldate = {2018-12-05},
	journal = {arXiv:1705.07263 [cs]},
	author = {Carlini, Nicholas and Wagner, David},
	month = may,
	year = {2017},
	note = {arXiv: 1705.07263},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv\:1705.07263 PDF:files/210/Carlini and Wagner - 2017 - Adversarial Examples Are Not Easily Detected Bypa.pdf:application/pdf;arXiv.org Snapshot:files/211/1705.html:text/html}
}

@article{simonyan_deep_2013,
	title = {Deep {Inside} {Convolutional} {Networks}: {Visualising} {Image} {Classification} {Models} and {Saliency} {Maps}},
	shorttitle = {Deep {Inside} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1312.6034},
	abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
	urldate = {2018-12-07},
	journal = {arXiv:1312.6034 [cs]},
	author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
	month = dec,
	year = {2013},
	note = {arXiv: 1312.6034},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1312.6034 PDF:files/214/Simonyan et al. - 2013 - Deep Inside Convolutional Networks Visualising Im.pdf:application/pdf;arXiv.org Snapshot:files/215/1312.html:text/html}
}