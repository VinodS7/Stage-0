
@article{szegedy_intriguing_2013,
	title = {Intriguing properties of neural networks},
	url = {http://arxiv.org/abs/1312.6199},
	abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.},
	language = {en},
	urldate = {2018-11-29},
	journal = {arXiv:1312.6199 [cs]},
	author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
	month = dec,
	year = {2013},
	note = {arXiv: 1312.6199},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Szegedy et al. - 2013 - Intriguing properties of neural networks.pdf:/home/vinod/Zotero/storage/V9S3Y7U6/Szegedy et al. - 2013 - Intriguing properties of neural networks.pdf:application/pdf}
}

@article{fogla_polymorphic_nodate,
	title = {Polymorphic {Blending} {Attacks}},
	abstract = {A very effective means to evade signature-based intrusion detection systems (IDS) is to employ polymorphic techniques to generate attack instances that do not share a ﬁxed signature. Anomaly-based intrusion detection systems provide good defense because existing polymorphic techniques can make the attack instances look different from each other, but cannot make them look like normal. In this paper we introduce a new class of polymorphic attacks, called polymorphic blending attacks, that can effectively evade byte frequencybased network anomaly IDS by carefully matching the statistics of the mutated attack instances to the normal proﬁles. The proposed polymorphic blending attacks can be viewed as a subclass of the mimicry attacks. We take a systematic approach to the problem and formally describe the algorithms and steps required to carry out such attacks. We not only show that such attacks are feasible but also analyze the hardness of evasion under different circumstances. We present detailed techniques using PAYL, a byte frequency-based anomaly IDS, as a case study and demonstrate that these attacks are indeed feasible. We also provide some insight into possible countermeasures that can be used as defense.},
	language = {en},
	author = {Fogla, Prahlad and Sharif, Monirul and Perdisci, Roberto and Kolesnikov, Oleg and Lee, Wenke},
	pages = {16},
	file = {Fogla et al. - Polymorphic Blending Attacks.pdf:/home/vinod/Zotero/storage/LZSWQWFU/Fogla et al. - Polymorphic Blending Attacks.pdf:application/pdf}
}

@article{biggio_evasion_2013,
	title = {Evasion {Attacks} against {Machine} {Learning} at {Test} {Time}},
	volume = {7908},
	url = {http://arxiv.org/abs/1708.06131},
	doi = {10.1007/978-3-642-40994-3_25},
	abstract = {In security-sensitive applications, the success of machine learning depends on a thorough vetting of their resistance to adversarial data. In one pertinent, well-motivated attack scenario, an adversary may attempt to evade a deployed system at test time by carefully manipulating attack samples. In this work, we present a simple but effective gradient-based approach that can be exploited to systematically assess the security of several, widely-used classification algorithms against evasion attacks. Following a recently proposed framework for security evaluation, we simulate attack scenarios that exhibit different risk levels for the classifier by increasing the attacker's knowledge of the system and her ability to manipulate attack samples. This gives the classifier designer a better picture of the classifier performance under evasion attacks, and allows him to perform a more informed model selection (or parameter setting). We evaluate our approach on the relevant security task of malware detection in PDF files, and show that such systems can be easily evaded. We also sketch some countermeasures suggested by our analysis.},
	urldate = {2018-11-29},
	journal = {arXiv:1708.06131 [cs]},
	author = {Biggio, Battista and Corona, Igino and Maiorca, Davide and Nelson, Blaine and Srndic, Nedim and Laskov, Pavel and Giacinto, Giorgio and Roli, Fabio},
	year = {2013},
	note = {arXiv: 1708.06131},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	pages = {387--402},
	file = {arXiv\:1708.06131 PDF:/home/vinod/Zotero/storage/8BAVLM6F/Biggio et al. - 2013 - Evasion Attacks against Machine Learning at Test T.pdf:application/pdf;arXiv.org Snapshot:/home/vinod/Zotero/storage/QTIN88L3/1708.html:text/html}
}

@article{hinton_distilling_2015,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	url = {http://arxiv.org/abs/1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	urldate = {2018-11-27},
	journal = {arXiv:1503.02531 [cs, stat]},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.02531},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv\:1503.02531 PDF:/home/vinod/Zotero/storage/FLIEFIM8/Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf:application/pdf;arXiv.org Snapshot:/home/vinod/Zotero/storage/WCNFAWIX/1503.html:text/html}
}

@article{goodfellow_explaining_2014,
	title = {Explaining and {Harnessing} {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1412.6572},
	abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
	urldate = {2018-11-27},
	journal = {arXiv:1412.6572 [cs, stat]},
	author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.6572},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1412.6572 PDF:/home/vinod/Zotero/storage/A5TNNEQS/Goodfellow et al. - 2014 - Explaining and Harnessing Adversarial Examples.pdf:application/pdf;arXiv.org Snapshot:/home/vinod/Zotero/storage/XG9L96N7/1412.html:text/html}
}

@article{alzantot_did_2018,
	title = {Did you hear that? {Adversarial} {Examples} {Against} {Automatic} {Speech} {Recognition}},
	shorttitle = {Did you hear that?},
	url = {http://arxiv.org/abs/1801.00554},
	abstract = {Speech is a common and effective way of communication between humans, and modern consumer devices such as smartphones and home hubs are equipped with deep learning based accurate automatic speech recognition to enable natural interaction between humans and machines. Recently, researchers have demonstrated powerful attacks against machine learning models that can fool them to produceincorrect results. However, nearly all previous research in adversarial attacks has focused on image recognition and object detection models. In this short paper, we present a first of its kind demonstration of adversarial attacks against speech classification model. Our algorithm performs targeted attacks with 87\% success by adding small background noise without having to know the underlying model parameter and architecture. Our attack only changes the least significant bits of a subset of audio clip samples, and the noise does not change 89\% the human listener's perception of the audio clip as evaluated in our human study.},
	urldate = {2018-11-27},
	journal = {arXiv:1801.00554 [cs]},
	author = {Alzantot, Moustafa and Balaji, Bharathan and Srivastava, Mani},
	month = jan,
	year = {2018},
	note = {arXiv: 1801.00554},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Computation and Language},
	file = {arXiv\:1801.00554 PDF:/home/vinod/Zotero/storage/M7ZYB59I/Alzantot et al. - 2018 - Did you hear that Adversarial Examples Against Au.pdf:application/pdf;arXiv.org Snapshot:/home/vinod/Zotero/storage/HEYDV9A9/1801.html:text/html}
}

@article{carlini_audio_2018,
	title = {Audio {Adversarial} {Examples}: {Targeted} {Attacks} on {Speech}-to-{Text}},
	shorttitle = {Audio {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1801.01944},
	abstract = {We construct targeted audio adversarial examples on automatic speech recognition. Given any audio waveform, we can produce another that is over 99.9\% similar, but transcribes as any phrase we choose (recognizing up to 50 characters per second of audio). We apply our white-box iterative optimization-based attack to Mozilla's implementation DeepSpeech end-to-end, and show it has a 100\% success rate. The feasibility of this attack introduce a new domain to study adversarial examples.},
	urldate = {2018-11-27},
	journal = {arXiv:1801.01944 [cs]},
	author = {Carlini, Nicholas and Wagner, David},
	month = jan,
	year = {2018},
	note = {arXiv: 1801.01944},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security, Computer Science - Artificial Intelligence},
	file = {arXiv\:1801.01944 PDF:/home/vinod/Zotero/storage/VVGMITH9/Carlini and Wagner - 2018 - Audio Adversarial Examples Targeted Attacks on Sp.pdf:application/pdf;arXiv.org Snapshot:/home/vinod/Zotero/storage/A44B8UFX/1801.html:text/html}
}

@article{makhzani_adversarial_2015,
	title = {Adversarial {Autoencoders}},
	url = {http://arxiv.org/abs/1511.05644},
	abstract = {In this paper, we propose the "adversarial autoencoder" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.},
	urldate = {2018-11-27},
	journal = {arXiv:1511.05644 [cs]},
	author = {Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.05644},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv\:1511.05644 PDF:/home/vinod/Zotero/storage/JF4HDL6N/Makhzani et al. - 2015 - Adversarial Autoencoders.pdf:application/pdf;arXiv.org Snapshot:/home/vinod/Zotero/storage/VTXMWWBE/1511.html:text/html}
}

@article{papernot_transferability_2016,
	title = {Transferability in {Machine} {Learning}: from {Phenomena} to {Black}-{Box} {Attacks} using {Adversarial} {Samples}},
	shorttitle = {Transferability in {Machine} {Learning}},
	url = {http://arxiv.org/abs/1605.07277},
	abstract = {Many machine learning models are vulnerable to adversarial examples: inputs that are specially crafted to cause a machine learning model to produce an incorrect output. Adversarial examples that affect one model often affect another model, even if the two models have different architectures or were trained on different training sets, so long as both models were trained to perform the same task. An attacker may therefore train their own substitute model, craft adversarial examples against the substitute, and transfer them to a victim model, with very little information about the victim. Recent work has further developed a technique that uses the victim model as an oracle to label a synthetic training set for the substitute, so the attacker need not even collect a training set to mount the attack. We extend these recent techniques using reservoir sampling to greatly enhance the efficiency of the training procedure for the substitute model. We introduce new transferability attacks between previously unexplored (substitute, victim) pairs of machine learning model classes, most notably SVMs and decision trees. We demonstrate our attacks on two commercial machine learning classification systems from Amazon (96.19\% misclassification rate) and Google (88.94\%) using only 800 queries of the victim model, thereby showing that existing machine learning approaches are in general vulnerable to systematic black-box attacks regardless of their structure.},
	urldate = {2018-11-27},
	journal = {arXiv:1605.07277 [cs]},
	author = {Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian},
	month = may,
	year = {2016},
	note = {arXiv: 1605.07277},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv\:1605.07277 PDF:/home/vinod/Zotero/storage/S8XYSL4U/Papernot et al. - 2016 - Transferability in Machine Learning from Phenomen.pdf:application/pdf;arXiv.org Snapshot:/home/vinod/Zotero/storage/PKJD22IH/1605.html:text/html}
}

@article{gu_towards_2014,
	title = {Towards {Deep} {Neural} {Network} {Architectures} {Robust} to {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1412.5068},
	abstract = {Recent work has shown deep neural networks (DNNs) to be highly susceptible to well-designed, small perturbations at the input layer, or so-called adversarial examples. Taking images as an example, such distortions are often imperceptible, but can result in 100\% mis-classification for a state of the art DNN. We study the structure of adversarial examples and explore network topology, pre-processing and training strategies to improve the robustness of DNNs. We perform various experiments to assess the removability of adversarial examples by corrupting with additional noise and pre-processing with denoising autoencoders (DAEs). We find that DAEs can remove substantial amounts of the adversarial noise. How- ever, when stacking the DAE with the original DNN, the resulting network can again be attacked by new adversarial examples with even smaller distortion. As a solution, we propose Deep Contractive Network, a model with a new end-to-end training procedure that includes a smoothness penalty inspired by the contractive autoencoder (CAE). This increases the network robustness to adversarial examples, without a significant performance penalty.},
	urldate = {2018-11-27},
	journal = {arXiv:1412.5068 [cs]},
	author = {Gu, Shixiang and Rigazio, Luca},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.5068},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1412.5068 PDF:/home/vinod/Zotero/storage/QYWVN5PI/Gu and Rigazio - 2014 - Towards Deep Neural Network Architectures Robust t.pdf:application/pdf;arXiv.org Snapshot:/home/vinod/Zotero/storage/KZI6IA6P/1412.html:text/html}
}

@article{papernot_distillation_2015,
	title = {Distillation as a {Defense} to {Adversarial} {Perturbations} against {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.04508},
	abstract = {Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content filters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95\% to less than 0.5\% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 10{\textasciicircum}30. We also find that distillation increases the average minimum number of features that need to be modified to create adversarial samples by about 800\% on one of the DNNs we tested.},
	urldate = {2018-11-27},
	journal = {arXiv:1511.04508 [cs, stat]},
	author = {Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.04508},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Cryptography and Security, Statistics - Machine Learning},
	file = {arXiv\:1511.04508 PDF:/home/vinod/Zotero/storage/M3LRCJEB/Papernot et al. - 2015 - Distillation as a Defense to Adversarial Perturbat.pdf:application/pdf;arXiv.org Snapshot:/home/vinod/Zotero/storage/BLAQV2S5/1511.html:text/html}
}

@article{madry_towards_2017,
	title = {Towards {Deep} {Learning} {Models} {Resistant} to {Adversarial} {Attacks}},
	url = {http://arxiv.org/abs/1706.06083},
	abstract = {Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models.},
	urldate = {2018-11-27},
	journal = {arXiv:1706.06083 [cs, stat]},
	author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.06083},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv\:1706.06083 PDF:/home/vinod/Zotero/storage/QVRSF8KG/Madry et al. - 2017 - Towards Deep Learning Models Resistant to Adversar.pdf:application/pdf;arXiv.org Snapshot:/home/vinod/Zotero/storage/YHUG5EJ8/1706.html:text/html}
}

@article{papernot_limitations_2015,
	title = {The {Limitations} of {Deep} {Learning} in {Adversarial} {Settings}},
	url = {http://arxiv.org/abs/1511.07528},
	abstract = {Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples: inputs crafted by adversaries with the intent of causing deep neural networks to misclassify. In this work, we formalize the space of adversaries against deep neural networks (DNNs) and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs. In an application to computer vision, we show that our algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97\% adversarial success rate while only modifying on average 4.02\% of the input features per sample. We then evaluate the vulnerability of different sample classes to adversarial perturbations by defining a hardness measure. Finally, we describe preliminary work outlining defenses against adversarial samples by defining a predictive measure of distance between a benign input and a target classification.},
	urldate = {2018-11-27},
	journal = {arXiv:1511.07528 [cs, stat]},
	author = {Papernot, Nicolas and McDaniel, Patrick and Jha, Somesh and Fredrikson, Matt and Celik, Z. Berkay and Swami, Ananthram},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.07528},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Cryptography and Security, Statistics - Machine Learning},
	file = {arXiv\:1511.07528 PDF:/home/vinod/Zotero/storage/4FLRV7RU/Papernot et al. - 2015 - The Limitations of Deep Learning in Adversarial Se.pdf:application/pdf;arXiv.org Snapshot:/home/vinod/Zotero/storage/CIJNQM3R/1511.html:text/html}
}

@article{metzen_detecting_2017,
	title = {On {Detecting} {Adversarial} {Perturbations}},
	url = {http://arxiv.org/abs/1702.04267},
	abstract = {Machine learning and deep learning in particular has advanced tremendously on perceptual tasks in recent years. However, it remains vulnerable against adversarial perturbations of the input that have been crafted specifically to fool the system while being quasi-imperceptible to a human. In this work, we propose to augment deep neural networks with a small "detector" subnetwork which is trained on the binary classification task of distinguishing genuine data from data containing adversarial perturbations. Our method is orthogonal to prior work on addressing adversarial perturbations, which has mostly focused on making the classification network itself more robust. We show empirically that adversarial perturbations can be detected surprisingly well even though they are quasi-imperceptible to humans. Moreover, while the detectors have been trained to detect only a specific adversary, they generalize to similar and weaker adversaries. In addition, we propose an adversarial attack that fools both the classifier and the detector and a novel training procedure for the detector that counteracts this attack.},
	urldate = {2018-12-05},
	journal = {arXiv:1702.04267 [cs, stat]},
	author = {Metzen, Jan Hendrik and Genewein, Tim and Fischer, Volker and Bischoff, Bastian},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.04267},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv\:1702.04267 PDF:/home/vinod/Zotero/storage/ZBJ7ULAZ/Metzen et al. - 2017 - On Detecting Adversarial Perturbations.pdf:application/pdf;arXiv.org Snapshot:/home/vinod/Zotero/storage/3BKH2FBF/1702.html:text/html}
}

@article{feinman_detecting_2017,
	title = {Detecting {Adversarial} {Samples} from {Artifacts}},
	url = {http://arxiv.org/abs/1703.00410},
	abstract = {Deep neural networks (DNNs) are powerful nonlinear architectures that are known to be robust to random perturbations of the input. However, these models are vulnerable to adversarial perturbations--small input changes crafted explicitly to fool the model. In this paper, we ask whether a DNN can distinguish adversarial samples from their normal and noisy counterparts. We investigate model confidence on adversarial samples by looking at Bayesian uncertainty estimates, available in dropout neural networks, and by performing density estimation in the subspace of deep features learned by the model. The result is a method for implicit adversarial detection that is oblivious to the attack algorithm. We evaluate this method on a variety of standard datasets including MNIST and CIFAR-10 and show that it generalizes well across different architectures and attacks. Our findings report that 85-93\% ROC-AUC can be achieved on a number of standard classification tasks with a negative class that consists of both normal and noisy samples.},
	urldate = {2018-12-05},
	journal = {arXiv:1703.00410 [cs, stat]},
	author = {Feinman, Reuben and Curtin, Ryan R. and Shintre, Saurabh and Gardner, Andrew B.},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.00410},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1703.00410 PDF:/home/vinod/Zotero/storage/MGRGLJ7X/Feinman et al. - 2017 - Detecting Adversarial Samples from Artifacts.pdf:application/pdf;arXiv.org Snapshot:/home/vinod/Zotero/storage/E4ETMM4L/1703.html:text/html}
}

@article{carlini_adversarial_2017,
	title = {Adversarial {Examples} {Are} {Not} {Easily} {Detected}: {Bypassing} {Ten} {Detection} {Methods}},
	shorttitle = {Adversarial {Examples} {Are} {Not} {Easily} {Detected}},
	url = {http://arxiv.org/abs/1705.07263},
	abstract = {Neural networks are known to be vulnerable to adversarial examples: inputs that are close to natural inputs but classified incorrectly. In order to better understand the space of adversarial examples, we survey ten recent proposals that are designed for detection and compare their efficacy. We show that all can be defeated by constructing new loss functions. We conclude that adversarial examples are significantly harder to detect than previously appreciated, and the properties believed to be intrinsic to adversarial examples are in fact not. Finally, we propose several simple guidelines for evaluating future proposed defenses.},
	urldate = {2018-12-05},
	journal = {arXiv:1705.07263 [cs]},
	author = {Carlini, Nicholas and Wagner, David},
	month = may,
	year = {2017},
	note = {arXiv: 1705.07263},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv\:1705.07263 PDF:/home/vinod/Zotero/storage/FPFCLYS8/Carlini and Wagner - 2017 - Adversarial Examples Are Not Easily Detected Bypa.pdf:application/pdf;arXiv.org Snapshot:/home/vinod/Zotero/storage/FBZW7PHW/1705.html:text/html}
}

@article{simonyan_deep_2013,
	title = {Deep {Inside} {Convolutional} {Networks}: {Visualising} {Image} {Classification} {Models} and {Saliency} {Maps}},
	shorttitle = {Deep {Inside} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1312.6034},
	abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
	urldate = {2018-12-07},
	journal = {arXiv:1312.6034 [cs]},
	author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
	month = dec,
	year = {2013},
	note = {arXiv: 1312.6034},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1312.6034 PDF:/home/vinod/Zotero/storage/B4C7FKJ9/Simonyan et al. - 2013 - Deep Inside Convolutional Networks Visualising Im.pdf:application/pdf;arXiv.org Snapshot:/home/vinod/Zotero/storage/NNVGUW4X/1312.html:text/html}
}

@article{carlini_towards_2016,
	title = {Towards {Evaluating} the {Robustness} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/1608.04644},
	abstract = {Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input \$x\$ and any target classification \$t\$, it is possible to find a new input \$x'\$ that is similar to \$x\$ but classified as \$t\$. This makes it difficult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks' ability to find adversarial examples from \$95{\textbackslash}\%\$ to \$0.5{\textbackslash}\%\$. In this paper, we demonstrate that defensive distillation does not significantly increase the robustness of neural networks by introducing three new attack algorithms that are successful on both distilled and undistilled neural networks with \$100{\textbackslash}\%\$ probability. Our attacks are tailored to three distance metrics used previously in the literature, and when compared to previous adversarial example generation algorithms, our attacks are often much more effective (and never worse). Furthermore, we propose using high-confidence adversarial examples in a simple transferability test we show can also be used to break defensive distillation. We hope our attacks will be used as a benchmark in future defense attempts to create neural networks that resist adversarial examples.},
	urldate = {2018-12-09},
	journal = {arXiv:1608.04644 [cs]},
	author = {Carlini, Nicholas and Wagner, David},
	month = aug,
	year = {2016},
	note = {arXiv: 1608.04644},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security},
	file = {arXiv\:1608.04644 PDF:/home/vinod/Zotero/storage/G3AHJDAE/Carlini and Wagner - 2016 - Towards Evaluating the Robustness of Neural Networ.pdf:application/pdf;arXiv.org Snapshot:/home/vinod/Zotero/storage/YXNPAKEB/1608.html:text/html}
}

@article{papernot_transferability_2016-1,
	title = {Transferability in {Machine} {Learning}: from {Phenomena} to {Black}-{Box} {Attacks} using {Adversarial} {Samples}},
	shorttitle = {Transferability in {Machine} {Learning}},
	url = {http://arxiv.org/abs/1605.07277},
	abstract = {Many machine learning models are vulnerable to adversarial examples: inputs that are specially crafted to cause a machine learning model to produce an incorrect output. Adversarial examples that affect one model often affect another model, even if the two models have different architectures or were trained on different training sets, so long as both models were trained to perform the same task. An attacker may therefore train their own substitute model, craft adversarial examples against the substitute, and transfer them to a victim model, with very little information about the victim. Recent work has further developed a technique that uses the victim model as an oracle to label a synthetic training set for the substitute, so the attacker need not even collect a training set to mount the attack. We extend these recent techniques using reservoir sampling to greatly enhance the efficiency of the training procedure for the substitute model. We introduce new transferability attacks between previously unexplored (substitute, victim) pairs of machine learning model classes, most notably SVMs and decision trees. We demonstrate our attacks on two commercial machine learning classification systems from Amazon (96.19\% misclassification rate) and Google (88.94\%) using only 800 queries of the victim model, thereby showing that existing machine learning approaches are in general vulnerable to systematic black-box attacks regardless of their structure.},
	urldate = {2018-12-09},
	journal = {arXiv:1605.07277 [cs]},
	author = {Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian},
	month = may,
	year = {2016},
	note = {arXiv: 1605.07277},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv\:1605.07277 PDF:/home/vinod/Zotero/storage/GPK9ZFFA/Papernot et al. - 2016 - Transferability in Machine Learning from Phenomen.pdf:application/pdf;arXiv.org Snapshot:/home/vinod/Zotero/storage/SSR2XN7D/1605.html:text/html}
}

@article{liu_delving_2016,
	title = {Delving into {Transferable} {Adversarial} {Examples} and {Black}-box {Attacks}},
	url = {http://arxiv.org/abs/1611.02770},
	abstract = {An intriguing property of deep neural networks is the existence of adversarial examples, which can transfer among different architectures. These transferable adversarial examples may severely hinder deep neural network-based applications. Previous works mostly study the transferability using small scale datasets. In this work, we are the first to conduct an extensive study of the transferability over large models and a large scale dataset, and we are also the first to study the transferability of targeted adversarial examples with their target labels. We study both non-targeted and targeted adversarial examples, and show that while transferable non-targeted adversarial examples are easy to find, targeted adversarial examples generated using existing approaches almost never transfer with their target labels. Therefore, we propose novel ensemble-based approaches to generating transferable adversarial examples. Using such approaches, we observe a large proportion of targeted adversarial examples that are able to transfer with their target labels for the first time. We also present some geometric studies to help understanding the transferable adversarial examples. Finally, we show that the adversarial examples generated using ensemble-based approaches can successfully attack Clarifai.com, which is a black-box image classification system.},
	urldate = {2018-12-09},
	journal = {arXiv:1611.02770 [cs]},
	author = {Liu, Yanpei and Chen, Xinyun and Liu, Chang and Song, Dawn},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.02770},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv\:1611.02770 PDF:/home/vinod/Zotero/storage/8TTIQTYS/Liu et al. - 2016 - Delving into Transferable Adversarial Examples and.pdf:application/pdf;arXiv.org Snapshot:/home/vinod/Zotero/storage/BUQ9IZED/1611.html:text/html}
}

@article{moosavi-dezfooli_universal_2016,
	title = {Universal adversarial perturbations},
	url = {http://arxiv.org/abs/1610.08401},
	abstract = {Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images.},
	urldate = {2018-12-09},
	journal = {arXiv:1610.08401 [cs, stat]},
	author = {Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Fawzi, Omar and Frossard, Pascal},
	month = oct,
	year = {2016},
	note = {arXiv: 1610.08401},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv\:1610.08401 PDF:/home/vinod/Zotero/storage/RPVVMRS9/Moosavi-Dezfooli et al. - 2016 - Universal adversarial perturbations.pdf:application/pdf;arXiv.org Snapshot:/home/vinod/Zotero/storage/X9R3RD75/1610.html:text/html}
}

@article{kurakin_adversarial_2016,
	title = {Adversarial examples in the physical world},
	url = {http://arxiv.org/abs/1607.02533},
	abstract = {Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.},
	urldate = {2018-12-09},
	journal = {arXiv:1607.02533 [cs, stat]},
	author = {Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
	month = jul,
	year = {2016},
	note = {arXiv: 1607.02533},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Cryptography and Security, Statistics - Machine Learning},
	file = {arXiv\:1607.02533 PDF:/home/vinod/Zotero/storage/4V4LPML4/Kurakin et al. - 2016 - Adversarial examples in the physical world.pdf:application/pdf;arXiv.org Snapshot:/home/vinod/Zotero/storage/X3JJHVSL/1607.html:text/html}
}

@article{athalye_synthesizing_2017,
	title = {Synthesizing {Robust} {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1707.07397},
	abstract = {Standard methods for generating adversarial examples for neural networks do not consistently fool neural network classifiers in the physical world due to a combination of viewpoint shifts, camera noise, and other natural transformations, limiting their relevance to real-world systems. We demonstrate the existence of robust 3D adversarial objects, and we present the first algorithm for synthesizing examples that are adversarial over a chosen distribution of transformations. We synthesize two-dimensional adversarial images that are robust to noise, distortion, and affine transformation. We apply our algorithm to complex three-dimensional objects, using 3D-printing to manufacture the first physical adversarial objects. Our results demonstrate the existence of 3D adversarial objects in the physical world.},
	urldate = {2018-12-09},
	journal = {arXiv:1707.07397 [cs]},
	author = {Athalye, Anish and Engstrom, Logan and Ilyas, Andrew and Kwok, Kevin},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.07397},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1707.07397 PDF:/home/vinod/Zotero/storage/JV6H8LZC/Athalye et al. - 2017 - Synthesizing Robust Adversarial Examples.pdf:application/pdf;arXiv.org Snapshot:/home/vinod/Zotero/storage/XLPRG8ZM/1707.html:text/html}
}

@article{eykholt_robust_2017,
	title = {Robust {Physical}-{World} {Attacks} on {Deep} {Learning} {Models}},
	url = {http://arxiv.org/abs/1707.08945},
	abstract = {Recent studies show that the state-of-the-art deep neural networks (DNNs) are vulnerable to adversarial examples, resulting from small-magnitude perturbations added to the input. Given that that emerging physical systems are using DNNs in safety-critical situations, adversarial examples could mislead these systems and cause dangerous situations.Therefore, understanding adversarial examples in the physical world is an important step towards developing resilient learning algorithms. We propose a general attack algorithm,Robust Physical Perturbations (RP2), to generate robust visual adversarial perturbations under different physical conditions. Using the real-world case of road sign classification, we show that adversarial examples generated using RP2 achieve high targeted misclassification rates against standard-architecture road sign classifiers in the physical world under various environmental conditions, including viewpoints. Due to the current lack of a standardized testing method, we propose a two-stage evaluation methodology for robust physical adversarial examples consisting of lab and field tests. Using this methodology, we evaluate the efficacy of physical adversarial manipulations on real objects. Witha perturbation in the form of only black and white stickers,we attack a real stop sign, causing targeted misclassification in 100\% of the images obtained in lab settings, and in 84.8\%of the captured video frames obtained on a moving vehicle(field test) for the target classifier.},
	urldate = {2018-12-09},
	journal = {arXiv:1707.08945 [cs]},
	author = {Eykholt, Kevin and Evtimov, Ivan and Fernandes, Earlence and Li, Bo and Rahmati, Amir and Xiao, Chaowei and Prakash, Atul and Kohno, Tadayoshi and Song, Dawn},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.08945},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv\:1707.08945 PDF:/home/vinod/Zotero/storage/UM4FZ9GJ/Eykholt et al. - 2017 - Robust Physical-World Attacks on Deep Learning Mod.pdf:application/pdf;arXiv.org Snapshot:/home/vinod/Zotero/storage/3Y8CNPHF/1707.html:text/html}
}

@article{kereliuk_deep_2015,
	title = {Deep {Learning} and {Music} {Adversaries}},
	volume = {17},
	issn = {1520-9210},
	doi = {10.1109/TMM.2015.2478068},
	abstract = {An adversary is an agent designed to make a classification system perform in some particular way, e.g., increase the probability of a false negative. Recent work builds adversaries for deep learning systems applied to image object recognition, exploiting the parameters of the system to find the minimal perturbation of the input image such that the system misclassifies it with high confidence. We adapt this approach to construct and deploy an adversary of deep learning systems applied to music content analysis. In our case, however, the system inputs are magnitude spectral frames, which require special care in order to produce valid input audio signals from network- derived perturbations . For two different train-test partitionings of two benchmark datasets, and two different architectures , we find that this adversary is very effective. We find that convolutional architectures are more robust compared to systems based on a majority vote over individually classified audio frames. Furthermore , we experiment with a new system that integrates an adversary into the training loop, but do not find that this improves the resilience of the system to new adversaries.},
	number = {11},
	journal = {IEEE Transactions on Multimedia},
	author = {Kereliuk, C. and Sturm, B. L. and Larsen, J.},
	month = nov,
	year = {2015},
	keywords = {learning (artificial intelligence), information retrieval, music, image classification, Training, Computer architecture, Neural networks, AEA-MIR content-based processing and music information retrieval, audio frame classification, audio signal, audio signal processing, Benchmark testing, classification system, content management, convolution, convolutional architecture, deep learning, deep learning system, image object recognition, Machine learning, magnitude spectral frame, music adversary, music content analysis, network derived perturbation, object recognition, Rhythm, training loop},
	pages = {2059--2071},
	file = {IEEE Xplore Abstract Record:/home/vinod/Zotero/storage/IFWMFFSA/7254179.html:text/html;IEEE Xplore Full Text PDF:/home/vinod/Zotero/storage/VY6BPEGD/Kereliuk et al. - 2015 - Deep Learning and Music Adversaries.pdf:application/pdf}
}

@article{grosse_statistical_2017,
	title = {On the ({Statistical}) {Detection} of {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1702.06280},
	abstract = {Machine Learning (ML) models are applied in a variety of tasks such as network intrusion detection or Malware classification. Yet, these models are vulnerable to a class of malicious inputs known as adversarial examples. These are slightly perturbed inputs that are classified incorrectly by the ML model. The mitigation of these adversarial inputs remains an open problem. As a step towards understanding adversarial examples, we show that they are not drawn from the same distribution than the original data, and can thus be detected using statistical tests. Using thus knowledge, we introduce a complimentary approach to identify specific inputs that are adversarial. Specifically, we augment our ML model with an additional output, in which the model is trained to classify all adversarial inputs. We evaluate our approach on multiple adversarial example crafting methods (including the fast gradient sign and saliency map methods) with several datasets. The statistical test flags sample sets containing adversarial inputs confidently at sample sizes between 10 and 100 data points. Furthermore, our augmented model either detects adversarial examples as outliers with high accuracy ({\textgreater} 80\%) or increases the adversary's cost - the perturbation added - by more than 150\%. In this way, we show that statistical properties of adversarial examples are essential to their detection.},
	urldate = {2018-12-09},
	journal = {arXiv:1702.06280 [cs, stat]},
	author = {Grosse, Kathrin and Manoharan, Praveen and Papernot, Nicolas and Backes, Michael and McDaniel, Patrick},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.06280},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security, Statistics - Machine Learning},
	file = {arXiv\:1702.06280 PDF:/home/vinod/Zotero/storage/ABMAHXAF/Grosse et al. - 2017 - On the (Statistical) Detection of Adversarial Exam.pdf:application/pdf;arXiv.org Snapshot:/home/vinod/Zotero/storage/V2FKKDLH/1702.html:text/html}
}

@article{metzen_detecting_2017-1,
	title = {On {Detecting} {Adversarial} {Perturbations}},
	url = {http://arxiv.org/abs/1702.04267},
	abstract = {Machine learning and deep learning in particular has advanced tremendously on perceptual tasks in recent years. However, it remains vulnerable against adversarial perturbations of the input that have been crafted specifically to fool the system while being quasi-imperceptible to a human. In this work, we propose to augment deep neural networks with a small "detector" subnetwork which is trained on the binary classification task of distinguishing genuine data from data containing adversarial perturbations. Our method is orthogonal to prior work on addressing adversarial perturbations, which has mostly focused on making the classification network itself more robust. We show empirically that adversarial perturbations can be detected surprisingly well even though they are quasi-imperceptible to humans. Moreover, while the detectors have been trained to detect only a specific adversary, they generalize to similar and weaker adversaries. In addition, we propose an adversarial attack that fools both the classifier and the detector and a novel training procedure for the detector that counteracts this attack.},
	urldate = {2018-12-09},
	journal = {arXiv:1702.04267 [cs, stat]},
	author = {Metzen, Jan Hendrik and Genewein, Tim and Fischer, Volker and Bischoff, Bastian},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.04267},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv\:1702.04267 PDF:/home/vinod/Zotero/storage/9ST2968I/Metzen et al. - 2017 - On Detecting Adversarial Perturbations.pdf:application/pdf;arXiv.org Snapshot:/home/vinod/Zotero/storage/SYQWJL2Z/1702.html:text/html}
}

@article{hendrycks_early_2016,
	title = {Early {Methods} for {Detecting} {Adversarial} {Images}},
	url = {http://arxiv.org/abs/1608.00530},
	abstract = {Many machine learning classifiers are vulnerable to adversarial perturbations. An adversarial perturbation modifies an input to change a classifier's prediction without causing the input to seem substantially different to human perception. We deploy three methods to detect adversarial images. Adversaries trying to bypass our detectors must make the adversarial image less pathological or they will fail trying. Our best detection method reveals that adversarial images place abnormal emphasis on the lower-ranked principal components from PCA. Other detectors and a colorful saliency map are in an appendix.},
	urldate = {2018-12-09},
	journal = {arXiv:1608.00530 [cs]},
	author = {Hendrycks, Dan and Gimpel, Kevin},
	month = aug,
	year = {2016},
	note = {arXiv: 1608.00530},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Cryptography and Security},
	file = {arXiv\:1608.00530 PDF:/home/vinod/Zotero/storage/AXMLDBEI/Hendrycks and Gimpel - 2016 - Early Methods for Detecting Adversarial Images.pdf:application/pdf;arXiv.org Snapshot:/home/vinod/Zotero/storage/XB9MDRU2/1608.html:text/html}
}

@article{bhagoji_enhancing_2017,
	title = {Enhancing {Robustness} of {Machine} {Learning} {Systems} via {Data} {Transformations}},
	url = {http://arxiv.org/abs/1704.02654},
	abstract = {We propose the use of dimensionality reduction as a defense against evasion attacks on ML classiﬁers. We present and investigate a strategy for incorporating dimensionality reduction via Principal Component Analysis to enhance the resilience of machine learning, targeting both the classiﬁcation and the training phase. We empirically evaluate and demonstrate the feasibility of dimensionality reduction of data as a defense mechanism against evasion attacks using multiple real-world datasets. Our key ﬁndings are that the defenses are (i) effective against strategic evasion attacks in the literature, increasing the resources required by an adversary for a successful attack by a factor of about two, (ii) applicable across a range of ML classiﬁers, including Support Vector Machines and Deep Neural Networks, and (iii) generalizable to multiple application domains, including image classiﬁcation, and human activity classiﬁcation.},
	language = {en},
	urldate = {2018-12-09},
	journal = {arXiv:1704.02654 [cs]},
	author = {Bhagoji, Arjun Nitin and Cullina, Daniel and Sitawarin, Chawin and Mittal, Prateek},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.02654},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {Bhagoji et al. - 2017 - Enhancing Robustness of Machine Learning Systems v.pdf:/home/vinod/Zotero/storage/W3LZ3CHD/Bhagoji et al. - 2017 - Enhancing Robustness of Machine Learning Systems v.pdf:application/pdf}
}

@inproceedings{li_adversarial_2017,
	address = {Venice},
	title = {Adversarial {Examples} {Detection} in {Deep} {Networks} with {Convolutional} {Filter} {Statistics}},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237877/},
	doi = {10.1109/ICCV.2017.615},
	abstract = {Deep learning has greatly improved visual recognition in recent years. However, recent research has shown that there exist many adversarial examples that can negatively impact the performance of such an architecture. This paper focuses on detecting those adversarial examples by analyzing whether they come from the same distribution as the normal examples. Instead of directly training a deep neural network to detect adversarials, a much simpler approach was proposed based on statistics on outputs from convolutional layers. A cascade classiﬁer was designed to efﬁciently detect adversarials. Furthermore, trained from one particular adversarial generating mechanism, the resulting classiﬁer can successfully detect adversarials from a completely different mechanism as well. The resulting classiﬁer is non-subdifferentiable, hence creates a difﬁculty for adversaries to attack by using the gradient of the classiﬁer. After detecting adversarial examples, we show that many of them can be recovered by simply performing a small average ﬁlter on the image. Those ﬁndings should lead to more insights about the classiﬁcation mechanisms in deep convolutional neural networks.},
	language = {en},
	urldate = {2018-12-09},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Li, Xin and Li, Fuxin},
	month = oct,
	year = {2017},
	pages = {5775--5783},
	file = {Li and Li - 2017 - Adversarial Examples Detection in Deep Networks wi.pdf:/home/vinod/Zotero/storage/PA25V2MI/Li and Li - 2017 - Adversarial Examples Detection in Deep Networks wi.pdf:application/pdf}
}

@article{viola_robust_nodate,
	title = {Robust {Real}-{Time} {Face} {Detection}},
	abstract = {This paper describes a face detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The ﬁrst is the introduction of a new image representation called the “Integral Image” which allows the features used by our detector to be computed very quickly. The second is a simple and efﬁcient classiﬁer which is built using the AdaBoost learning algorithm (Freund and Schapire, 1995) to select a small number of critical visual features from a very large set of potential features. The third contribution is a method for combining classiﬁers in a “cascade” which allows background regions of the image to be quickly discarded while spending more computation on promising face-like regions. A set of experiments in the domain of face detection is presented. The system yields face detection performance comparable to the best previous systems (Sung and Poggio, 1998; Rowley et al., 1998; Schneiderman and Kanade, 2000; Roth et al., 2000). Implemented on a conventional desktop, face detection proceeds at 15 frames per second.},
	language = {en},
	author = {Viola, Paul and Jones, Michael J},
	pages = {18},
	file = {Viola and Jones - Robust Real-Time Face Detection.pdf:/home/vinod/Zotero/storage/72DYU9HU/Viola and Jones - Robust Real-Time Face Detection.pdf:application/pdf}
}

@article{feinman_detecting_2017-1,
	title = {Detecting {Adversarial} {Samples} from {Artifacts}},
	url = {http://arxiv.org/abs/1703.00410},
	abstract = {Deep neural networks (DNNs) are powerful nonlinear architectures that are known to be robust to random perturbations of the input. However, these models are vulnerable to adversarial perturbations--small input changes crafted explicitly to fool the model. In this paper, we ask whether a DNN can distinguish adversarial samples from their normal and noisy counterparts. We investigate model confidence on adversarial samples by looking at Bayesian uncertainty estimates, available in dropout neural networks, and by performing density estimation in the subspace of deep features learned by the model. The result is a method for implicit adversarial detection that is oblivious to the attack algorithm. We evaluate this method on a variety of standard datasets including MNIST and CIFAR-10 and show that it generalizes well across different architectures and attacks. Our findings report that 85-93\% ROC-AUC can be achieved on a number of standard classification tasks with a negative class that consists of both normal and noisy samples.},
	urldate = {2018-12-09},
	journal = {arXiv:1703.00410 [cs, stat]},
	author = {Feinman, Reuben and Curtin, Ryan R. and Shintre, Saurabh and Gardner, Andrew B.},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.00410},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1703.00410 PDF:/home/vinod/Zotero/storage/NN4P7F95/Feinman et al. - 2017 - Detecting Adversarial Samples from Artifacts.pdf:application/pdf;arXiv.org Snapshot:/home/vinod/Zotero/storage/H5HEXSLL/1703.html:text/html}
}

@article{gretton_kernel_2012,
	title = {A {Kernel} {Two}-{Sample} {Test}},
	volume = {13},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v13/gretton12a.html},
	number = {Mar},
	urldate = {2018-12-09},
	journal = {Journal of Machine Learning Research},
	author = {Gretton, Arthur and Borgwardt, Karsten M. and Rasch, Malte J. and Schölkopf, Bernhard and Smola, Alexander},
	year = {2012},
	pages = {723--773},
	file = {Full Text PDF:/home/vinod/Zotero/storage/WPWAGSG2/Gretton et al. - 2012 - A Kernel Two-Sample Test.pdf:application/pdf;Snapshot:/home/vinod/Zotero/storage/72CVL4TA/gretton12a.html:text/html}
}

@book{anton_elementary_1987,
	address = {New York},
	title = {Elementary linear algebra},
	isbn = {978-0-471-84819-6 978-0-471-85223-0},
	language = {en},
	publisher = {Wiley},
	author = {Anton, Howard},
	year = {1987},
	note = {OCLC: 13580207},
	file = {Anton - 1987 - Elementary linear algebra.pdf:/home/vinod/Zotero/storage/P9KWE468/Anton - 1987 - Elementary linear algebra.pdf:application/pdf}
}

@article{shlens_tutorial_2014,
	title = {A {Tutorial} on {Principal} {Component} {Analysis}},
	url = {http://arxiv.org/abs/1404.1100},
	abstract = {Principal component analysis (PCA) is a mainstay of modern data analysis - a black box that is widely used but (sometimes) poorly understood. The goal of this paper is to dispel the magic behind this black box. This manuscript focuses on building a solid intuition for how and why principal component analysis works. This manuscript crystallizes this knowledge by deriving from simple intuitions, the mathematics behind PCA. This tutorial does not shy away from explaining the ideas informally, nor does it shy away from the mathematics. The hope is that by addressing both aspects, readers of all levels will be able to gain a better understanding of PCA as well as the when, the how and the why of applying this technique.},
	language = {en},
	urldate = {2018-12-09},
	journal = {arXiv:1404.1100 [cs, stat]},
	author = {Shlens, Jonathon},
	month = apr,
	year = {2014},
	note = {arXiv: 1404.1100},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Shlens - 2014 - A Tutorial on Principal Component Analysis.pdf:/home/vinod/Zotero/storage/7N9TECQT/Shlens - 2014 - A Tutorial on Principal Component Analysis.pdf:application/pdf}
}

@article{zhou_chord_2015,
	title = {{CHORD} {DETECTION} {USING} {DEEP} {LEARNING}},
	abstract = {In this paper, we utilize deep learning to learn high-level features for audio chord detection. The learned features, obtained by a deep network in bottleneck architecture, give promising results and outperform state-of-the-art systems. We present and evaluate the results for various methods and conﬁgurations, including input pre-processing, a bottleneck architecture, and SVMs vs. HMMs for chord classiﬁcation.},
	language = {en},
	author = {Zhou, Xinquan and Lerch, Alexander},
	year = {2015},
	pages = {7},
	file = {Zhou and Lerch - 2015 - CHORD DETECTION USING DEEP LEARNING.pdf:/home/vinod/Zotero/storage/4VPZPUAT/Zhou and Lerch - 2015 - CHORD DETECTION USING DEEP LEARNING.pdf:application/pdf}
}

@article{costa_evaluation_2017,
	title = {An evaluation of {Convolutional} {Neural} {Networks} for music classification using spectrograms},
	volume = {52},
	issn = {1568-4946},
	url = {http://www.sciencedirect.com/science/article/pii/S1568494616306421},
	doi = {10.1016/j.asoc.2016.12.024},
	abstract = {Music genre recognition based on visual representation has been successfully explored over the last years. Classifiers trained with textural descriptors (e.g., Local Binary Patterns, Local Phase Quantization, and Gabor filters) extracted from the spectrograms have achieved state-of-the-art results on several music datasets. In this work, though, we argue that we can go further with the time-frequency analysis through the use of representation learning. To show that, we compare the results obtained with a Convolutional Neural Network (CNN) with the results obtained by using handcrafted features and SVM classifiers. In addition, we have performed experiments fusing the results obtained with learned features and handcrafted features to assess the complementarity between these representations for the music classification task. Experiments were conducted on three music databases with distinct characteristics, specifically a western music collection largely used in research benchmarks (ISMIR 2004 Database), a collection of Latin American music (LMD database), and a collection of field recordings of ethnic African music. Our experiments show that the CNN compares favorably to other classifiers in several scenarios, hence, it is a very interesting alternative for music genre recognition. Considering the African database, the CNN surpassed the handcrafted representations and also the state-of-the-art by a margin. In the case of the LMD database, the combination of CNN and Robust Local Binary Pattern achieved a recognition rate of 92\%, which to the best of our knowledge, is the best result (using an artist filter) on this dataset so far. On the ISMIR 2004 dataset, although the CNN did not improve the state of the art, it performed better than the classifiers based individually on other kind of features.},
	urldate = {2018-12-14},
	journal = {Applied Soft Computing},
	author = {Costa, Yandre M. G. and Oliveira, Luiz S. and Silla, Carlos N.},
	month = mar,
	year = {2017},
	keywords = {Pattern recognition, Music genre recognition, Neural network applications},
	pages = {28--38},
	file = {ScienceDirect Full Text PDF:/home/vinod/Zotero/storage/A3T69JUI/Costa et al. - 2017 - An evaluation of Convolutional Neural Networks for.pdf:application/pdf;ScienceDirect Snapshot:/home/vinod/Zotero/storage/36592B8E/S1568494616306421.html:text/html}
}

@article{dieleman_multiscale_nodate,
	title = {{MULTISCALE} {APPROACHES} {TO} {MUSIC} {AUDIO} {FEATURE} {LEARNING}},
	abstract = {Content-based music information retrieval tasks are typically solved with a two-stage approach: features are extracted from music audio signals, and are then used as input to a regressor or classiﬁer. These features can be engineered or learned from data. Although the former approach was dominant in the past, feature learning has started to receive more attention from the MIR community in recent years. Recent results in feature learning indicate that simple algorithms such as K-means can be very effective, sometimes surpassing more complicated approaches based on restricted Boltzmann machines, autoencoders or sparse coding. Furthermore, there has been increased interest in multiscale representations of music audio recently. Such representations are more versatile because music audio exhibits structure on multiple timescales, which are relevant for different MIR tasks to varying degrees. We develop and compare three approaches to multiscale audio feature learning using the spherical K-means algorithm. We evaluate them in an automatic tagging task and a similarity metric learning task on the Magnatagatune dataset.},
	language = {en},
	author = {Dieleman, Sander and Schrauwen, Benjamin},
	pages = {6},
	file = {Dieleman and Schrauwen - MULTISCALE APPROACHES TO MUSIC AUDIO FEATURE LEARN.pdf:/home/vinod/Zotero/storage/JNL2DKH6/Dieleman and Schrauwen - MULTISCALE APPROACHES TO MUSIC AUDIO FEATURE LEARN.pdf:application/pdf}
}

@inproceedings{wang_improving_2014,
	address = {Orlando, Florida, USA},
	title = {Improving {Content}-based and {Hybrid} {Music} {Recommendation} using {Deep} {Learning}},
	isbn = {978-1-4503-3063-3},
	url = {http://dl.acm.org/citation.cfm?doid=2647868.2654940},
	doi = {10.1145/2647868.2654940},
	abstract = {Existing content-based music recommendation systems typically employ a two-stage approach. They ﬁrst extract traditional audio content features such as Mel-frequency cepstral coeﬃcients and then predict user preferences. However, these traditional features, originally not created for music recommendation, cannot capture all relevant information in the audio and thus put a cap on recommendation performance. Using a novel model based on deep belief network and probabilistic graphical model, we unify the two stages into an automated process that simultaneously learns features from audio content and makes personalized recommendations. Compared with existing deep learning based models, our model outperforms them in both the warm-start and cold-start stages without relying on collaborative ﬁltering (CF). We then present an eﬃcient hybrid method to seamlessly integrate the automatically learnt features and CF. Our hybrid method not only signiﬁcantly improves the performance of CF but also outperforms the traditional feature based hybrid method.},
	language = {en},
	urldate = {2018-12-14},
	booktitle = {Proceedings of the {ACM} {International} {Conference} on {Multimedia} - {MM} '14},
	publisher = {ACM Press},
	author = {Wang, Xinxi and Wang, Ye},
	year = {2014},
	pages = {627--636},
	file = {Wang and Wang - 2014 - Improving Content-based and Hybrid Music Recommend.pdf:/home/vinod/Zotero/storage/7RDX99AS/Wang and Wang - 2014 - Improving Content-based and Hybrid Music Recommend.pdf:application/pdf}
}

@article{mishra_understanding_2018,
	title = {{UNDERSTANDING} {A} {DEEP} {MACHINE} {LISTENING} {MODEL} {THROUGH} {FEATURE} {INVERSION}},
	abstract = {Methods for interpreting machine learning models can help one understand their global and/or local behaviours, and thereby improve them. In this work, we apply a global analysis method to a machine listening model, which essentially inverts the features generated in a model back into an interpretable form like a sonogram. We demonstrate this method for a state-of-the-art singing voice detection model. We train up-convolutional neural networks to invert the feature generated at each layer of the model. The results suggest that the deepest fully connected layer of the model does not preserve temporal and harmonic structures, but that the inverted features from the deepest convolutional layer do. Moreover, a qualitative analysis of a large number of inputs suggests that the deepest layer in the model learns a decision function as the information it preserves depends on the class label associated with an input.},
	language = {en},
	author = {Mishra, Saumitra and Sturm, Bob L and Dixon, Simon},
	year = {2018},
	pages = {8},
	file = {Mishra et al. - 2018 - UNDERSTANDING A DEEP MACHINE LISTENING MODEL THROU.pdf:/home/vinod/Zotero/storage/2X2A9UNN/Mishra et al. - 2018 - UNDERSTANDING A DEEP MACHINE LISTENING MODEL THROU.pdf:application/pdf}
}

@inproceedings{dieleman_end--end_2014,
	title = {End-to-end learning for music audio},
	doi = {10.1109/ICASSP.2014.6854950},
	abstract = {Content-based music information retrieval tasks have traditionally been solved using engineered features and shallow processing architectures. In recent years, there has been increasing interest in using feature learning and deep architectures instead, thus reducing the required engineering effort and the need for prior knowledge. However, this new approach typically still relies on mid-level representations of music audio, e.g. spectrograms, instead of raw audio signals. In this paper, we investigate whether it is possible to apply feature learning directly to raw audio signals. We train convolutional neural networks using both approaches and compare their performance on an automatic tagging task. Although they do not outperform a spectrogram-based approach, the networks are able to autonomously discover frequency decompositions from raw audio, as well as phase-and translation-invariant feature representations.},
	booktitle = {2014 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Dieleman, S. and Schrauwen, B.},
	month = may,
	year = {2014},
	keywords = {learning (artificial intelligence), music, Music information retrieval, music information retrieval, Computer architecture, Spectrogram, Neural networks, automatic tagging, automatic tagging task, content-based music information retrieval tasks, content-based retrieval, Convolution, convolutional neural networks, convolutional neural networks training, end-to-end learning, feature learning, frequency decompositions, music audio, phase-and translation-invariant feature representations, raw audio, spectrogram-based approach, Speech},
	pages = {6964--6968},
	file = {IEEE Xplore Abstract Record:/home/vinod/Zotero/storage/JHWBERK7/6854950.html:text/html;IEEE Xplore Full Text PDF:/home/vinod/Zotero/storage/ZB6PFWR3/Dieleman and Schrauwen - 2014 - End-to-end learning for music audio.pdf:application/pdf}
}

@book{humphrey_moving_nodate,
	title = {Moving {Beyond} {Feature} {Design}: {Deep} {Architectures} and {Automatic} {Feature} {Learning} in {Music} {Informatics}},
	shorttitle = {Moving {Beyond} {Feature} {Design}},
	abstract = {The short history of content-based music informatics research is dominated by hand-crafted feature design, and our community has grown admittedly complacent with a few de facto standards. Despite commendable progress in many areas, it is increasingly apparent that our efforts are yielding diminishing returns. This deceleration is largely due to the tandem of heuristic feature design and shallow processing architectures. We systematically discard hopefully irrelevant information while simultaneously calling upon creativity, intuition, or sheer luck to craft useful representations, gradually evolving complex, carefully tuned systems to address specific tasks. While other disciplines have seen the benefits of deep learning, it has only recently started to be explored in our field. By reviewing deep architectures and feature learning, we hope to raise awareness in our community about alternative approaches to solving MIR challenges, new and old alike. 1.},
	author = {Humphrey, Eric J. and Bello, Juan Pablo and Lecun, Yann},
	file = {Citeseer - Full Text PDF:/home/vinod/Zotero/storage/X5E99I3E/Humphrey et al. - Moving Beyond Feature Design Deep Architectures a.pdf:application/pdf;Citeseer - Snapshot:/home/vinod/Zotero/storage/CVUIVMYQ/summary.html:text/html}
}

@article{gabrielsson_music_2003,
	title = {Music {Performance} {Research} at the {Millennium}},
	volume = {31},
	issn = {0305-7356, 1741-3087},
	url = {http://journals.sagepub.com/doi/10.1177/03057356030313002},
	doi = {10.1177/03057356030313002},
	language = {en},
	number = {3},
	urldate = {2018-12-14},
	journal = {Psychology of Music},
	author = {Gabrielsson, Alf},
	month = jul,
	year = {2003},
	pages = {221--272},
	file = {Gabrielsson - 2003 - Music Performance Research at the Millennium.pdf:/home/vinod/Zotero/storage/BWVR7C5J/Gabrielsson - 2003 - Music Performance Research at the Millennium.pdf:application/pdf}
}

@misc{noauthor_c4dm_nodate,
	title = {C4DM {Poster} {Template}},
	url = {https://www.overleaf.com/project/5c126a8fb798eb47ca92e624},
	abstract = {An online LaTeX editor that's easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
	language = {en},
	urldate = {2018-12-13},
	file = {Snapshot:/home/vinod/Zotero/storage/FFLLTJQ4/5c126a8fb798eb47ca92e624.pdf:application/pdf}
}

@article{bosch_comparison_2012,
	title = {A {COMPARISON} {OF} {SOUND} {SEGREGATION} {TECHNIQUES} {FOR} {PREDOMINANT} {INSTRUMENT} {RECOGNITION} {IN} {MUSICAL} {AUDIO} {SIGNALS}},
	abstract = {The authors address the identification of predominant music instruments in polytimbral audio by previously dividing the original signal into several streams. Several strategies are evaluated, ranging from low to high complexity with respect to the segregation algorithm and models used for classification. The dataset of interest is built from professionally produced recordings, which typically pose problems to state-of-art source separation algorithms. The recognition results are improved a 19\% with a simple sound segregation pre-step using only panning information, in comparison to the original algorithm. In order to further improve the results, we evaluated the use of a complex source separation as a pre-step. The results showed that the performance was only enhanced if the recognition models are trained with the features extracted from the separated audio streams. In this way, the typical errors of state-of-art separation algorithms are acknowledged, and the performance of the original instrument recognition algorithm is improved in up to 32\%.},
	language = {en},
	author = {Bosch, Juan J and Janer, Jordi and Fuhrmann, Ferdinand and Herrera, Perfecto},
	year = {2012},
	pages = {6},
	file = {Bosch et al. - 2012 - A COMPARISON OF SOUND SEGREGATION TECHNIQUES FOR P.pdf:/home/vinod/Zotero/storage/WCSQSLBR/Bosch et al. - 2012 - A COMPARISON OF SOUND SEGREGATION TECHNIQUES FOR P.pdf:application/pdf}
}

@article{bosch_comparison_2012-1,
	title = {A {COMPARISON} {OF} {SOUND} {SEGREGATION} {TECHNIQUES} {FOR} {PREDOMINANT} {INSTRUMENT} {RECOGNITION} {IN} {MUSICAL} {AUDIO} {SIGNALS}},
	abstract = {The authors address the identification of predominant music instruments in polytimbral audio by previously dividing the original signal into several streams. Several strategies are evaluated, ranging from low to high complexity with respect to the segregation algorithm and models used for classification. The dataset of interest is built from professionally produced recordings, which typically pose problems to state-of-art source separation algorithms. The recognition results are improved a 19\% with a simple sound segregation pre-step using only panning information, in comparison to the original algorithm. In order to further improve the results, we evaluated the use of a complex source separation as a pre-step. The results showed that the performance was only enhanced if the recognition models are trained with the features extracted from the separated audio streams. In this way, the typical errors of state-of-art separation algorithms are acknowledged, and the performance of the original instrument recognition algorithm is improved in up to 32\%.},
	language = {en},
	author = {Bosch, Juan J and Janer, Jordi and Fuhrmann, Ferdinand and Herrera, Perfecto},
	year = {2012},
	pages = {6},
	file = {Bosch et al. - 2012 - A COMPARISON OF SOUND SEGREGATION TECHNIQUES FOR P.pdf:/home/vinod/Zotero/storage/9TV457CV/Bosch et al. - 2012 - A COMPARISON OF SOUND SEGREGATION TECHNIQUES FOR P.pdf:application/pdf}
}

@article{sturm_gtzan_2014,
	title = {The {GTZAN} dataset: {Its} contents, its faults, their effects on evaluation, and its future use},
	volume = {43},
	issn = {0929-8215, 1744-5027},
	shorttitle = {The {GTZAN} dataset},
	url = {http://arxiv.org/abs/1306.1461},
	doi = {10.1080/09298215.2014.894533},
	abstract = {The GTZAN dataset appears in at least 100 published works, and is the most-used public dataset for evaluation in machine listening research for music genre recognition (MGR). Our recent work, however, shows GTZAN has several faults (repetitions, mislabelings, and distortions), which challenge the interpretability of any result derived using it. In this article, we disprove the claims that all MGR systems are affected in the same ways by these faults, and that the performances of MGR systems in GTZAN are still meaningfully comparable since they all face the same faults. We identify and analyze the contents of GTZAN, and provide a catalog of its faults. We review how GTZAN has been used in MGR research, and find few indications that its faults have been known and considered. Finally, we rigorously study the effects of its faults on evaluating five different MGR systems. The lesson is not to banish GTZAN, but to use it with consideration of its contents.},
	number = {2},
	urldate = {2018-12-13},
	journal = {Journal of New Music Research},
	author = {Sturm, Bob L.},
	month = apr,
	year = {2014},
	note = {arXiv: 1306.1461},
	keywords = {Computer Science - Sound},
	pages = {147--172},
	file = {arXiv\:1306.1461 PDF:/home/vinod/Zotero/storage/6FP79SSN/Sturm - 2014 - The GTZAN dataset Its contents, its faults, their.pdf:application/pdf;arXiv.org Snapshot:/home/vinod/Zotero/storage/6JT6CJJY/1306.html:text/html}
}

@article{gong_crafting_2017,
	title = {Crafting {Adversarial} {Examples} {For} {Speech} {Paralinguistics} {Applications}},
	url = {http://arxiv.org/abs/1711.03280},
	abstract = {Computational paralinguistic analysis is increasingly being used in a wide range of applications, including securitysensitive applications such as speaker veriﬁcation, deceptive speech detection, and medical diagnostics. While state-ofthe-art machine learning techniques, such as deep neural networks, can provide robust and accurate speech analysis, they are susceptible to adversarial attacks. In this work, we propose a novel end-to-end scheme to generate adversarial examples by perturbing directly the raw waveform of an audio recording rather than speciﬁc acoustic features. Our experiments show that the proposed adversarial perturbation can lead to a signiﬁcant performance drop of state-of-the-art deep neural networks, while only minimally impairing the audio quality.},
	language = {en},
	urldate = {2018-12-11},
	journal = {arXiv:1711.03280 [cs, eess, stat]},
	author = {Gong, Yuan and Poellabauer, Christian},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.03280},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security, Statistics - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Gong and Poellabauer - 2017 - Crafting Adversarial Examples For Speech Paralingu.pdf:/home/vinod/Zotero/storage/J285YCVA/Gong and Poellabauer - 2017 - Crafting Adversarial Examples For Speech Paralingu.pdf:application/pdf}
}

@article{choi_explaining_2016,
	title = {Explaining {Deep} {Convolutional} {Neural} {Networks} on {Music} {Classification}},
	url = {http://arxiv.org/abs/1607.02444},
	abstract = {Deep convolutional neural networks (CNNs) have been actively adopted in the ﬁeld of music information retrieval, e.g. genre classiﬁcation, mood detection, and chord recognition. However, the process of learning and prediction is little understood, particularly when it is applied to spectrograms. We introduce auralisation of a CNN to understand its underlying mechanism, which is based on a deconvolution procedure introduced in [2]. Auralisation of a CNN is converting the learned convolutional features that are obtained from deconvolution into audio signals. In the experiments and discussions, we explain trained features of a 5-layer CNN based on the deconvolved spectrograms and auralised signals. The pairwise correlations per layers with varying different musical attributes are also investigated to understand the evolution of the learnt features. It is shown that in the deep layers, the features are learnt to capture textures, the patterns of continuous distributions, rather than shapes of lines.},
	language = {en},
	urldate = {2018-12-14},
	journal = {arXiv:1607.02444 [cs]},
	author = {Choi, Keunwoo and Fazekas, George and Sandler, Mark},
	month = jul,
	year = {2016},
	note = {arXiv: 1607.02444},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multimedia, Computer Science - Sound},
	file = {Choi et al. - 2016 - Explaining Deep Convolutional Neural Networks on M.pdf:/home/vinod/Zotero/storage/62D3LNUU/Choi et al. - 2016 - Explaining Deep Convolutional Neural Networks on M.pdf:application/pdf}
}

@misc{noauthor_nsynth_nodate,
	title = {The {NSynth} {Dataset}},
	url = {https://magenta.tensorflow.org/datasets/nsynth},
	abstract = {A large-scale and high-quality dataset of annotated musical notes.},
	language = {en},
	urldate = {2018-12-14},
	journal = {Magenta},
	file = {Snapshot:/home/vinod/Zotero/storage/S6DBEK8T/nsynth.html:text/html}
}

@article{engel_neural_2017,
	title = {Neural {Audio} {Synthesis} of {Musical} {Notes} with {WaveNet} {Autoencoders}},
	url = {http://arxiv.org/abs/1704.01279},
	abstract = {Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.},
	urldate = {2018-12-14},
	journal = {arXiv:1704.01279 [cs]},
	author = {Engel, Jesse and Resnick, Cinjon and Roberts, Adam and Dieleman, Sander and Eck, Douglas and Simonyan, Karen and Norouzi, Mohammad},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.01279},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Sound},
	file = {arXiv\:1704.01279 PDF:/home/vinod/Zotero/storage/98C72FDR/Engel et al. - 2017 - Neural Audio Synthesis of Musical Notes with WaveN.pdf:application/pdf;arXiv.org Snapshot:/home/vinod/Zotero/storage/M8VSMJ9G/1704.html:text/html}
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	number = {8},
	urldate = {2018-12-14},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	pages = {1735--1780},
	file = {Snapshot:/home/vinod/Zotero/storage/AU72PK9L/neco.1997.9.8.html:text/html}
}

@inproceedings{zhang_improving_2014,
	title = {Improving deep neural network acoustic models using generalized maxout networks},
	doi = {10.1109/ICASSP.2014.6853589},
	abstract = {Recently, maxout networks have brought significant improvements to various speech recognition and computer vision tasks. In this paper we introduce two new types of generalized maxout units, which we call p-norm and soft-maxout. We investigate their performance in Large Vocabulary Continuous Speech Recognition (LVCSR) tasks in various languages with 10 hours and 60 hours of data, and find that the p-norm generalization of maxout consistently performs well. Because, in our training setup, we sometimes see instability during training when training unbounded-output nonlinearities such as these, we also present a method to control that instability. This is the “normalization layer”, which is a nonlinearity that scales down all dimensions of its input in order to stop the average squared output from exceeding one. The performance of our proposed nonlinearities are compared with maxout, rectified linear units (ReLU), tanh units, and also with a discriminatively trained SGMM/HMM system, and our p-norm units with p equal to 2 are found to perform best.},
	booktitle = {2014 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Zhang, X. and Trmal, J. and Povey, D. and Khudanpur, S.},
	month = may,
	year = {2014},
	keywords = {Acoustic Modeling, Acoustics, computer vision task, Deep Learning, deep neural network acoustic models, generalisation (artificial intelligence), generalized maxout networks, large vocabulary continuous speech recognition, LVCSR task, Maxout Networks, neural nets, Neural networks, normalization layer, p-norm generalization, p-norm units, rectified linear units, ReLU, soft-maxout, Speech, Speech processing, speech recognition, Speech recognition, Speech Recognition, Training, Training data, unbounded-output nonlinearities},
	pages = {215--219},
	file = {IEEE Xplore Abstract Record:/home/vinod/Zotero/storage/TMKQVEUN/6853589.html:text/html;IEEE Xplore Full Text PDF:/home/vinod/Zotero/storage/XTJR9VIK/Zhang et al. - 2014 - Improving deep neural network acoustic models usin.pdf:application/pdf}
}

@article{maas_rectier_nodate,
	title = {Rectiﬁer {Nonlinearities} {Improve} {Neural} {Network} {Acoustic} {Models}},
	abstract = {Deep neural network acoustic models produce substantial gains in large vocabulary continuous speech recognition systems. Emerging work with rectiﬁed linear (ReL) hidden units demonstrates additional gains in ﬁnal system performance relative to more commonly used sigmoidal nonlinearities. In this work, we explore the use of deep rectiﬁer networks as acoustic models for the 300 hour Switchboard conversational speech recognition task. Using simple training procedures without pretraining, networks with rectiﬁer nonlinearities produce 2\% absolute reductions in word error rates over their sigmoidal counterparts. We analyze hidden layer representations to quantify diﬀerences in how ReL units encode inputs as compared to sigmoidal units. Finally, we evaluate a variant of the ReL unit with a gradient more amenable to optimization in an attempt to further improve deep rectiﬁer networks.},
	language = {en},
	author = {Maas, Andrew L and Hannun, Awni Y and Ng, Andrew Y},
	pages = {6},
	file = {Maas et al. - Rectiﬁer Nonlinearities Improve Neural Network Aco.pdf:/home/vinod/Zotero/storage/792DWFIS/Maas et al. - Rectiﬁer Nonlinearities Improve Neural Network Aco.pdf:application/pdf}
}